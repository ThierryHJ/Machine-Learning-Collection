{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy sample prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Fine-tuning data (prompt-response pairs)\n",
    "sft_data = [\n",
    "    {\"prompt\": \"Explain rain to a child.\", \"response\": \"Rain is water falling from the clouds when they get too heavy.\"},\n",
    "    {\"prompt\": \"Define happiness simply.\", \"response\": \"Happiness is feeling joyful and good inside.\"},\n",
    "]\n",
    "\n",
    "# Comparison data for reward model (prompt, preferred_response, other_response)\n",
    "rm_data = [\n",
    "    {\n",
    "        \"prompt\": \"Explain rain to a child.\",\n",
    "        \"preferred\": \"Rain is water falling from the clouds when they get too heavy.\",\n",
    "        \"other\": \"Rain happens when water evaporates from the ground.\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# New prompt for PPO stage\n",
    "ppo_prompt = \"What is sunshine?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Prepare SFT data\n",
    "def tokenize_fn(example):\n",
    "    prompt = example['prompt']\n",
    "    response = example['response']\n",
    "    text = prompt + \" \" + response + tokenizer.eos_token\n",
    "    return tokenizer(text, truncation=True, padding=\"max_length\", max_length=50)\n",
    "\n",
    "tokenized_sft_data = [tokenize_fn(d) for d in sft_data]\n",
    "\n",
    "class SFTDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.data[idx]['input_ids'])\n",
    "        labels = input_ids.clone()\n",
    "        return {'input_ids': input_ids, 'labels': labels}\n",
    "\n",
    "train_dataset = SFTDataset(tokenized_sft_data)\n",
    "\n",
    "# SFT training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', num_train_epochs=3, per_device_train_batch_size=2,\n",
    "    learning_rate=5e-5, logging_steps=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reward Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "# Reward model initialized from GPT-2\n",
    "reward_model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=1)\n",
    "\n",
    "# Prepare RM data (pairwise preference)\n",
    "def tokenize_rm(example):\n",
    "    prompt = example['prompt']\n",
    "    preferred = prompt + \" \" + example['preferred']\n",
    "    other = prompt + \" \" + example['other']\n",
    "    return {\n",
    "        'preferred': tokenizer(preferred, truncation=True, padding='max_length', max_length=50, return_tensors='pt'),\n",
    "        'other': tokenizer(other, truncation=True, padding='max_length', max_length=50, return_tensors='pt')\n",
    "    }\n",
    "\n",
    "tokenized_rm_data = [tokenize_rm(d) for d in rm_data]\n",
    "\n",
    "# RM training step (simplified)\n",
    "optimizer = torch.optim.Adam(reward_model.parameters(), lr=1e-5)\n",
    "reward_model.train()\n",
    "\n",
    "for epoch in range(3):\n",
    "    for sample in tokenized_rm_data:\n",
    "        # Get tokenized inputs for preferred and other responses\n",
    "        preferred_input = sample['preferred']['input_ids']\n",
    "        other_input = sample['other']['input_ids']\n",
    "\n",
    "        # Compute scalar reward scores\n",
    "        preferred_scores = reward_model(preferred_input).logits\n",
    "        other_scores = reward_model(other_input).logits\n",
    "\n",
    "        # Calculate pairwise ranking loss matching the RM formula\n",
    "        loss = -torch.log(torch.sigmoid(preferred_scores - other_scores)).mean()\n",
    "\n",
    "        # update gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Reinforcement Learning via PPO (RLHF)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize PPO policy from supervised fine-tuned model (SFT)\n",
    "policy_model = model\n",
    "policy_optimizer = torch.optim.Adam(policy_model.parameters(), lr=1e-5)\n",
    "reward_model.eval()\n",
    "\n",
    "ppo_prompt = \"What is sunshine?\"\n",
    "tokenized_prompt = tokenizer(ppo_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response using current policy model π_φ(y|x)\n",
    "output_ids = policy_model.generate(\n",
    "    tokenized_prompt['input_ids'], max_length=50, num_return_sequences=1\n",
    ")\n",
    "generated_response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Tokenize complete prompt+response for reward calculation\n",
    "tokenized_response = tokenizer(ppo_prompt + \" \" + generated_response, return_tensors=\"pt\")\n",
    "\n",
    "# Compute reward r_θ(x,y) using reward model\n",
    "with torch.no_grad():\n",
    "    reward_score = reward_model(tokenized_response['input_ids']).logits.item()\n",
    "\n",
    "# Compute the KL penalty to regularize against original SFT policy\n",
    "with torch.no_grad():\n",
    "    # Probability distribution from current policy π_φ\n",
    "    current_policy_logits = policy_model(tokenized_prompt['input_ids']).logits\n",
    "    current_policy_probs = F.softmax(current_policy_logits, dim=-1)\n",
    "\n",
    "    # Probability distribution from SFT policy π_θ\n",
    "    sft_policy_logits = model(tokenized_prompt['input_ids']).logits\n",
    "    sft_policy_probs = F.softmax(sft_policy_logits, dim=-1)\n",
    "\n",
    "    # KL divergence\n",
    "    kl_divergence = F.kl_div(current_policy_probs.log(), sft_policy_probs.log(), reduction='batchmean')\n",
    "\n",
    "# Combine reward and KL-penalty into final reward as defined in PPO\n",
    "beta = 0.01  # KL penalty coefficient\n",
    "reward = reward_score - beta * kl_divergence.item()\n",
    "\n",
    "# PPO policy loss (maximize reward → minimize negative reward)\n",
    "loss = -reward\n",
    "\n",
    "# Policy update step (gradient descent)\n",
    "policy_optimizer.zero_grad()\n",
    "loss.backward()\n",
    "policy_optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
