{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "562642eb-8326-4829-bf5e-3df839912246",
   "metadata": {},
   "source": [
    "# Implement Scaled Dot-Product Attention in PyTorch\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Your task is to **implement the Scaled Dot-Product Attention mechanism**, including the **Query (Q), Key (K), and Value (V) transformations**, in PyTorch. This mechanism is a fundamental operation in **Transformer-based models**, as introduced in the **Attention Is All You Need** paper (Vaswani et al., 2017). \n",
    "\n",
    "You will build a **self-attention module** and apply it to a **machine translation** task using the **Multi30k dataset** (English-German sentence pairs).\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Background\n",
    "\n",
    "The **Scaled Dot-Product Attention** computes attention weights and aggregates relevant information from different parts of the input sequence. Given input embeddings, the mechanism generates Query, Key, and Value matrices, then applies attention to learn contextual relationships.\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "For each sequence, the **attention score** is computed as:\n",
    "\n",
    "$$e_{ij} = \\frac{Q_i K_j^T}{\\sqrt{d_k}}$$\n",
    "\n",
    "where:\n",
    "- **\\(Q = X W_Q\\)** → Query matrix\n",
    "- **\\(K = X W_K\\)** → Key matrix\n",
    "- **\\(V = X W_V\\)** → Value matrix\n",
    "\n",
    "where:\n",
    "- \\( X \\) is the input embeddings.\n",
    "- \\( W_Q, W_K, W_V \\) are learnable weight matrices.\n",
    "- \\( d_k \\) is the hidden size of keys/queries (used for scaling).\n",
    "\n",
    "The normalized attention weights are:\n",
    "\n",
    "$$alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T} \\exp(e_{ik})}$$\n",
    "\n",
    "The final attention output is:\n",
    "\n",
    "\n",
    "$$O_i = \\sum_{j=1}^{T} \\alpha_{ij} V_j$$\n",
    "\n",
    "where:\n",
    "- \\( O \\) is the output sequence after applying attention.\n",
    "\n",
    "---\n",
    "\n",
    "## **Task Requirements**\n",
    "\n",
    "### 1️⃣ Implement **Scaled Dot-Product Attention with Query, Key, and Value Transformations**\n",
    "- Compute **Query, Key, and Value matrices** from the input.\n",
    "- Apply **dot-product attention**:\n",
    "  - Compute attention scores.\n",
    "  - Normalize with softmax.\n",
    "  - Aggregate values based on attention scores.\n",
    "\n",
    "### 2️⃣ Integrate into a Simple **Self-Attention Model**\n",
    "- Use the **Multi30k dataset** for English-German translation.\n",
    "- Implement a **Transformer-style attention block** using your scaled dot-product attention module.\n",
    "- Build a basic **encoder-decoder architecture** for sequence-to-sequence translation.\n",
    "\n",
    "### 3️⃣ Handle Variable-Length Sequences\n",
    "- Implement **masking** to ignore padding tokens.\n",
    "\n",
    "---\n",
    "\n",
    "## **Constraints**\n",
    "- The input to the attention module should be of shape:\n",
    "\n",
    "$$X \\in \\mathbb{R}^{(B, T, d)}$$\n",
    "\n",
    "where:\n",
    "- **\\(B\\)** is the batch size.\n",
    "- **\\(T\\)** is the sequence length.\n",
    "- **\\(d\\)** is the hidden dimension of embeddings.\n",
    "\n",
    "- The Query, Key, and Value matrices should be computed using:\n",
    "\n",
    "$$Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V$$\n",
    "\n",
    "where \\( W_Q, W_K, W_V \\) are **trainable weight matrices**.\n",
    "\n",
    "---\n",
    "\n",
    "## **💡 Hints**\n",
    "1. **Define Learnable Weight Matrices**:\n",
    "   - Use `nn.Linear` layers to generate Q, K, and V.\n",
    "   - Example: `self.W_Q = nn.Linear(d_model, d_k, bias=False)`\n",
    "\n",
    "2. **Apply Dot-Product Attention**:\n",
    "   - Compute scores: `torch.matmul(Q, K.transpose(-2, -1))`\n",
    "   - Scale by $$sqrt{d_k}$$: `scores /= math.sqrt(d_k)`\n",
    "   - Apply `torch.softmax(scores, dim=-1)`\n",
    "\n",
    "3. **Handle Padding Masks**:\n",
    "   - Mask **padding tokens** before softmax using `masked_fill(mask == 0, -inf)`\n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Example Implementation**\n",
    "\n",
    "### **1️⃣ Implement Scaled Dot-Product Attention**\n",
    "```python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9e942465-64fd-4797-923d-72a5d134aecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, num_heads=8):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Learnable linear layers for Q, K, V transformations\n",
    "        self.Q = nn.Linear(d_model, d_model)\n",
    "        self.K = nn.Linear(d_model, d_model)\n",
    "        self.V = nn.Linear(d_model, d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model, bias=False)  # Final output projection\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, X, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        X: (batch_size, seq_len, d_model) - Input embeddings\n",
    "        mask: (batch_size, seq_len, seq_len) - Optional padding mask\n",
    "\n",
    "        Returns:\n",
    "        - Attention output (batch_size, seq_len, d_k)\n",
    "        - Attention weights (batch_size, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        B, T, D = X.shape\n",
    "        \n",
    "        # Compute Query, Key, and Value \n",
    "        # (batch_size, seq_len, head, d_k) -> (batch_size, head, seq_len, d_k)\n",
    "        Query = self.Q(X).reshape(B, T, self.num_heads, self.d_k).transpose(1, 2) #batch_size * head * token_size * k\n",
    "        Key = self.K(X).reshape(B, T, self.num_heads, self.d_k).transpose(1, 2) \n",
    "        Value = self.V(X).reshape(B, T, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        # (batch_size, head, seq_len, seq_len)\n",
    "        attention_scores = torch.matmul(Query, Key.transpose(-1, -2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask (optional)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('inf'))\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.softmax(attention_scores)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        output = torch.matmul(attn_weights, Value) # (batch_size, head, seq_len, d_k)\n",
    "        output = output.transpose(1, 2).reshape(B, T, D) # (batch_size, seq_len, d_model)\n",
    "        output = self.W_O(output)\n",
    "    \n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5e35e7c6-10a7-46a5-8f74-eaca0e8ffb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v=32):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k\n",
    "\n",
    "        # Query, Key, and Value transformations\n",
    "        self.W_Q = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_K = nn.Linear(d_model, d_k, bias=False)\n",
    "        self.W_V = nn.Linear(d_model, d_v, bias=False)\n",
    "        self.W_O = nn.Linear(d_v, d_model, bias=False)  # Final output projection\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, X, mask=None):\n",
    "        batch_size, seq_len, _ = X.shape\n",
    "\n",
    "        # Compute Query, Key, and Value projections\n",
    "        Q = self.W_Q(X)  # Shape: (batch_size, seq_len, d_k)\n",
    "        K = self.W_K(X)  # Shape: (batch_size, seq_len, d_k)\n",
    "        V = self.W_V(X)  # Shape: (batch_size, seq_len, d_v)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # Apply mask (optional)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.softmax(attention_scores)  # Shape: (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # Compute weighted sum of values\n",
    "        output = torch.matmul(attn_weights, V)  # Shape: (batch_size, seq_len, d_v)\n",
    "\n",
    "        # Project back to d_model\n",
    "        # output = self.W_O(output)  # Shape: (batch_size, seq_len, d_model)\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec58365f-13e3-4927-bd08-563a806e11c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 3 required positional arguments: 'query', 'key', and 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m batch_size, seq_len, d_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m512\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m ScaledDotProductAttention(d_model\u001b[38;5;241m=\u001b[39minput_dim, num_heads\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[0;32m----> 4\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torchsummary/torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 3 required positional arguments: 'query', 'key', and 'value'"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "batch_size, seq_len, d_model = 16, 10, 512\n",
    "model = ScaledDotProductAttention(d_model=input_dim, num_heads=num_heads)\n",
    "summary(model, input_size=(32, 8, 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0c2362ae-9008-45f0-9538-65cb41045d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.1077\n",
      "Epoch [11/50], Loss: 0.0815\n",
      "Epoch [21/50], Loss: 0.0254\n",
      "Epoch [31/50], Loss: 0.0071\n",
      "Epoch [41/50], Loss: 0.0022\n",
      "Sample Output Shape: torch.Size([16, 10, 512])\n",
      "Sample Attention Weights Shape: torch.Size([16, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training MultiHeadAttention with Toy Dataset\n",
    "def train_attention_with_toy_dataset():\n",
    "    batch_size, seq_len, d_model = 16, 10, 512\n",
    "    num_heads = 8\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    attention = ScaledDotProductAttention(d_model=d_model, num_heads=num_heads)\n",
    "    optimizer = torch.optim.Adam(attention.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Initialize Toy Dataset and DataLoader\n",
    "    class ToySequenceDataset(Dataset):\n",
    "        def __init__(self, num_samples=500, sequence_length=10, input_dim=512):\n",
    "            self.sequence_length = sequence_length\n",
    "            self.input_dim = input_dim\n",
    "            self.data = torch.rand(num_samples, sequence_length, input_dim)  # Shape: (num_samples, seq_length, input_dim)\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "\n",
    "    toy_dataset = ToySequenceDataset(num_samples=500)\n",
    "    train_loader = DataLoader(toy_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        attention.train()\n",
    "        total_loss = 0\n",
    "        for sequences in train_loader:\n",
    "            query, key, value = sequences, sequences, sequences\n",
    "            mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, attn_weights = attention(query, key, value, mask)\n",
    "            loss = criterion(outputs, sequences)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "    \n",
    "    # Testing the trained model\n",
    "    attention.eval()\n",
    "    with torch.no_grad():\n",
    "        test_sequences = next(iter(train_loader))\n",
    "        test_outputs, test_attn_weights = attention(test_sequences, test_sequences, test_sequences, mask)\n",
    "        print(\"Sample Output Shape:\", test_outputs.shape)  # Expected: (batch_size, sequence_length, d_model)\n",
    "        print(\"Sample Attention Weights Shape:\", test_attn_weights.shape)  # Expected: (batch_size, num_heads, sequence_length, sequence_length)\n",
    "\n",
    "train_attention_with_toy_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2adf5f-e605-4b9f-a843-7e4068d2f997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
