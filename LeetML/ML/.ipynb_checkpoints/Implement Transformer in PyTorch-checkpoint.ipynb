{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9b098f3-8a58-4e1f-a0d0-dd104aaafd1d",
   "metadata": {},
   "source": [
    "# Implement Transformer in PyTorch\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Your task is to **implement a Transformer model**, including the **encoder and decoder architecture**, in PyTorch. This model follows the design outlined in the **Attention Is All You Need** paper (Vaswani et al., 2017).\n",
    "\n",
    "You will build a **Transformer-based sequence-to-sequence model** and apply it to a **machine translation task** using the **Multi30k dataset** (English-German sentence pairs).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Background\n",
    "\n",
    "The **Transformer model** consists of:\n",
    "1. **Encoder**:\n",
    "   - Embeds the input sequence.\n",
    "   - Applies multiple layers of **self-attention** and **feed-forward networks**.\n",
    "   \n",
    "2. **Decoder**:\n",
    "   - Takes the encoder output.\n",
    "   - Uses **masked self-attention** to process its own input.\n",
    "   - Applies **encoder-decoder attention** to attend to encoder outputs.\n",
    "   - Passes through a feed-forward network.\n",
    "\n",
    "3. **Final Linear & Softmax Layer**:\n",
    "   - Produces word probabilities for translation.\n",
    "\n",
    "### **Mathematical Formulation**\n",
    "A single **Multi-Head Attention** layer is defined as:\n",
    "\n",
    "$${Attention}(Q, K, V) = \\text{softmax} \\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)$$\n",
    "\n",
    "where:\n",
    "- \\( Q, K, V \\) are Query, Key, and Value matrices.\n",
    "- \\( d_k \\) is the hidden size of queries/keys (used for scaling).\n",
    "\n",
    "The **multi-head attention** mechanism applies multiple attention layers in parallel:\n",
    "\n",
    "$${MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O$$\n",
    "\n",
    "where:\n",
    "- Each **head** independently computes attention using different learnable projections.\n",
    "\n",
    "---\n",
    "\n",
    "## **Task Requirements**\n",
    "\n",
    "### 1ï¸âƒ£ Implement a **Transformer Encoder**\n",
    "- Implement:\n",
    "  - **Token embeddings** and **positional encodings**.\n",
    "  - **Multi-head self-attention**.\n",
    "  - **Feed-forward network** with layer normalization.\n",
    "\n",
    "### 2ï¸âƒ£ Implement a **Transformer Decoder**\n",
    "- Implement:\n",
    "  - **Masked multi-head attention** to prevent peeking.\n",
    "  - **Encoder-decoder attention** to attend to encoder outputs.\n",
    "\n",
    "### 3ï¸âƒ£ Implement a **Transformer block**\n",
    "- with position encoding\n",
    "\n",
    "---\n",
    "\n",
    "## **Constraints**\n",
    "- The input tensors should be shaped as:\n",
    "\n",
    "$$X \\in \\mathbb{R}^{(B, T)}$$\n",
    "\n",
    "where:\n",
    "- **\\(B\\)** is the batch size.\n",
    "- **\\(T\\)** is the sequence length.\n",
    "\n",
    "- The Transformer model should follow:\n",
    "  - **Embedding dimension**: \\(d_{model} = 512\\)\n",
    "  - **Number of heads**: \\(h = 8\\)\n",
    "  - **Feed-forward dimension**: \\(d_{ff} = 2048\\)\n",
    "  - **Layers**: 6 Encoder + 6 Decoder\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ’¡ Hints**\n",
    "1. **Use PyTorch's `nn.MultiheadAttention`**:\n",
    "   - Example: `self.attn = nn.MultiheadAttention(embed_dim=512, num_heads=8)`\n",
    "\n",
    "2. **Implement Positional Encoding**:\n",
    "   - Use sinusoidal functions to encode word positions.\n",
    "\n",
    "3. **Apply Layer Normalization & Residual Connections**:\n",
    "   - `nn.LayerNorm(d_model)`\n",
    "\n",
    "4. **Mask Future Tokens in the Decoder**:\n",
    "   - Use `torch.triu(torch.ones(T, T), diagonal=1).bool()`\n",
    "\n",
    "---\n",
    "\n",
    "## **ðŸ“Œ Example Implementation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71d4d8f0-172f-4afd-be64-7437904542ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "########################################\n",
    "# 1) Transformer Encoder Block\n",
    "########################################\n",
    "\n",
    "# Keep original comment\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # define attention layer, ffn, layer norm layers, and dropout\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        self.feed_forward_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),  # Fix nn.Relu() -> nn.ReLU()\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        # We'll name these consistently\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        # 1. multi-head self attention\n",
    "        attn_output, _ = self.attention_layer(src, src, src, attn_mask=src_mask)\n",
    "        # 2. add + norm\n",
    "        output = self.layer_norm_1(src + self.dropout(attn_output))\n",
    "        # 3. feed forward\n",
    "        ffn_output = self.feed_forward_layer(output)\n",
    "        # 4. add + norm\n",
    "        output = self.layer_norm_2(output + self.dropout(ffn_output))\n",
    "        return output\n",
    "\n",
    "########################################\n",
    "# 2) Transformer Decoder Block\n",
    "########################################\n",
    "\n",
    "# Keep original comment\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # define attention layer, cross attention layer, ffn, layer norm layers, and dropout\n",
    "        self.self_attn_layer = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        self.cross_attn_layer = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads)\n",
    "        self.feed_forward_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),  # Fix nn.Relu() -> nn.ReLU()\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # 1. multi-head self attention\n",
    "        self_attn_output, _ = self.self_attn_layer(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        # 2. add + norm\n",
    "        tgt = self.layer_norm(tgt + self.dropout(self_attn_output))\n",
    "        # 3. multi-head cross attention\n",
    "        cross_attn_output, _ = self.cross_attn_layer(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        # 4. add + norm\n",
    "        output = self.layer_norm_2(tgt + self.dropout(cross_attn_output))\n",
    "        # 5. feed forward\n",
    "        ffn_output = self.feed_forward_layer(output)\n",
    "        # 6. add + norm\n",
    "        output = self.layer_norm_3(output + self.dropout(ffn_output))\n",
    "        return output\n",
    "\n",
    "########################################\n",
    "# Positional Encoding\n",
    "########################################\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"pe\", torch.zeros(1, seq_len, d_model))\n",
    "        position = torch.arange(0, seq_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        self.pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        self.pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "########################################\n",
    "# 3) Implement Transformer Model\n",
    "########################################\n",
    "\n",
    "# Keep original comment\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, num_heads=8, d_ff=2048, num_layers=6):\n",
    "        super().__init__()\n",
    "        # 1. embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        # 2. positional encoder\n",
    "        self.pe = PositionalEncoding(seq_len=5000, d_model=d_model)  # arbitrary large seq_len\n",
    "        # 3. encoder\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerEncoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        # 4. decoder\n",
    "        self.decoder = nn.ModuleList([\n",
    "            TransformerDecoderLayer(d_model, num_heads, d_ff) for _ in range(num_layers)\n",
    "        ])\n",
    "        # 5. output layer\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)  # fix to nn.Linear\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # decoder\n",
    "        src = self.embedding(src)\n",
    "        src = self.pe(src)\n",
    "        for encoder_layer in self.encoder:\n",
    "            src = encoder_layer(src)\n",
    "\n",
    "        # encoder\n",
    "        tgt = self.embedding(tgt)\n",
    "        tgt = self.pe(tgt)\n",
    "        for decoder_layer in self.decoder:\n",
    "            tgt = decoder_layer(tgt, src)\n",
    "\n",
    "        return self.fc_out(tgt)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad51fef7-6cbe-466b-b6d7-ba64244aae0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([16, 10, 100])\n"
     ]
    }
   ],
   "source": [
    "def test_transformer():\n",
    "    batch_size = 16\n",
    "    seq_len = 10\n",
    "    vocab_size = 100\n",
    "    model = Transformer(vocab_size=vocab_size, d_model=32, num_heads=4, d_ff=64, num_layers=2)\n",
    "\n",
    "    # Create random integer tensors for src and tgt\n",
    "    src = torch.randint(0, vocab_size, (batch_size, seq_len))  # (B, T)\n",
    "    tgt = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    # Pass them through the model\n",
    "    out = model(src, tgt)  # (B, T, vocab_size)\n",
    "    print(\"Output shape:\", out.shape)  # Expect (16, 10, 100)\n",
    "\n",
    "    # The model is basically untrained, so we just ensure it runs.\n",
    "\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "217e069d-c741-4a63-9646-1141d0d4ccf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 9.4341\n",
      "Epoch [11/50], Loss: 8.7746\n",
      "Epoch [21/50], Loss: 8.7186\n",
      "Epoch [31/50], Loss: 8.7036\n",
      "Epoch [41/50], Loss: 8.6840\n",
      "Transformer Output Shape: torch.Size([16, 20, 10000])\n"
     ]
    }
   ],
   "source": [
    "# Training Transformer Model with Toy Dataset\n",
    "def train_transformer_with_toy_dataset():\n",
    "    vocab_size = 10000\n",
    "    d_model = 512\n",
    "    seq_len = 20\n",
    "    batch_size = 16\n",
    "    num_heads = 8\n",
    "    num_layers = 6\n",
    "    d_ff = 2048\n",
    "    max_len = 5000\n",
    "    num_epochs = 50\n",
    "    learning_rate = 0.001\n",
    "    \n",
    "    model = Transformer(vocab_size, d_model, num_heads, d_ff, num_layers)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Initialize Toy Dataset and DataLoader\n",
    "    class ToySequenceDataset(Dataset):\n",
    "        def __init__(self, num_samples=500, sequence_length=20, vocab_size=10000):\n",
    "            self.sequence_length = sequence_length\n",
    "            self.vocab_size = vocab_size\n",
    "            self.data = torch.randint(0, vocab_size, (num_samples, sequence_length))\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            return self.data[idx]\n",
    "\n",
    "    toy_dataset = ToySequenceDataset(num_samples=500)\n",
    "    train_loader = DataLoader(toy_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for sequences in train_loader:\n",
    "            src, tgt = sequences, sequences\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src, tgt)\n",
    "            loss = criterion(output.view(-1, vocab_size), tgt.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Testing the trained model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_sequences = next(iter(train_loader))\n",
    "        test_outputs = model(test_sequences, test_sequences)\n",
    "        print(\"Transformer Output Shape:\", test_outputs.shape)  # Expected: (batch_size, seq_len, vocab_size)\n",
    "\n",
    "train_transformer_with_toy_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6565db3-a856-4cc5-af3f-b7a9ba175a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be84076-b28d-4576-8155-b3e6e9bcdaf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
