{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc7851ff-2105-45c2-b7a4-11b8e41b7af9",
   "metadata": {},
   "source": [
    "# Implement an RNN in PyTorch\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "You are tasked with implementing a **Recurrent Neural Network (RNN)** in PyTorch to process sequential data. The goal is to classify human activities based on motion sensor readings from a real-world dataset.\n",
    "\n",
    "### Dataset\n",
    "Use the **Human Activity Recognition (HAR) dataset** from the UCI Machine Learning Repository:  \n",
    "[UCI HAR Dataset](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones).  \n",
    "This dataset contains accelerometer and gyroscope readings from smartphones, labeled with six different activities:  \n",
    "- Walking  \n",
    "- Walking Upstairs  \n",
    "- Walking Downstairs  \n",
    "- Sitting  \n",
    "- Standing  \n",
    "- Laying  \n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### Define the RNN Model:\n",
    "- Implement an **RNN-based classifier** that takes time-series sensor data as input and predicts the corresponding human activity.\n",
    "- Use a **recurrent layer** (e.g., `nn.RNN`, `nn.LSTM`, or `nn.GRU`) to process sequential input.\n",
    "- Add a **fully connected layer** (`nn.Linear`) to map the RNN output to the final classification.\n",
    "\n",
    "#### Implement the Forward Pass:\n",
    "- Process the input sequence through the RNN layer.\n",
    "- Extract the final hidden state to make a prediction.\n",
    "- Pass the final hidden state through the fully connected layer to obtain the output logits.\n",
    "\n",
    "### Constraints\n",
    "- The input data is a sequence of shape `(batch_size, sequence_length, input_dim)`, where:\n",
    "  - `sequence_length` is the number of timesteps (128 in this dataset).\n",
    "  - `input_dim` is the number of sensor features (9 in this dataset).\n",
    "  - The model should output a prediction for each sample in the batch.\n",
    "- Use appropriate configurations for:\n",
    "  - **Hidden units** (`hidden_size`).\n",
    "  - **Number of recurrent layers** (`num_layers`).\n",
    "  - **Dropout** for regularization.\n",
    "  \n",
    "### Mathematical Formulation\n",
    "\n",
    "A basic RNN computes the hidden state **\\( h_t \\)** at time step **\\( t \\)** using the formula:\n",
    "\n",
    "$$h_t = \\tanh(W_h h_{t-1} + W_x x_t + b)$$\n",
    "\n",
    "where:\n",
    "- \\( x_t \\) is the input at time **\\( t \\)**.\n",
    "- \\( h_{t-1} \\) is the hidden state from the previous step.\n",
    "- \\( W_h, W_x, b \\) are learnable parameters.\n",
    "\n",
    "The final output is obtained using:\n",
    "\n",
    "$$y = \\text{softmax}(W_y h_T + b_y)$$\n",
    "\n",
    "where \\( h_T \\) is the hidden state at the final timestep \\( T \\).\n",
    "\n",
    "### ðŸ’¡ Hints\n",
    "1. Define an RNN layer (`self.rnn = nn.RNN(...)`) in `__init__`.\n",
    "2. Use `self.fc = nn.Linear(hidden_size, num_classes)` for classification.\n",
    "3. Extract the last hidden state and pass it through `self.fc` in `forward()`.\n",
    "4. Consider using **GRU** (`nn.GRU`) or **LSTM** (`nn.LSTM`) for improved performance.\n",
    "\n",
    "### Example Input and Output\n",
    "\n",
    "#### Input:\n",
    "A batch of accelerometer and gyroscope readings:\n",
    "\n",
    "```python\n",
    "torch.Size([32, 128, 9])  # batch_size=32, sequence_length=128, input_dim=9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8ce7d6-0cb1-484d-9a04-985875d7d8a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad2dacba-f795-4e13-8ebf-8d3c38921ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "756015ee-c6f1-4b96-978e-688ecaa4b1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        # input -> hidden \n",
    "        self.w_xh = nn.Linear(input_size, hidden_size)\n",
    "        # hidden -> hidden\n",
    "        self.w_hh = nn.Linear(hidden_size, hidden_size)\n",
    "        # Final Fully Connected Layer for Output\n",
    "        self.fcn = nn.Linear(hidden_size, num_classes)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, input_size = x.shape\n",
    "        h_t = torch.randn((batch_size, self.hidden_size))\n",
    "        \n",
    "        # for each seq, generate output and hidden\n",
    "        for i in range(seq_length):\n",
    "            x_t = x[:, i, :]\n",
    "            h_t = self.tanh(self.w_xh(x_t) + self.w_hh(h_t))\n",
    "\n",
    "        # return output and hidden of the final seq\n",
    "        y_t = self.fcn(h_t)\n",
    "        return y_t\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bf107ac-f537-4b9a-9a76-9726c02953b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                   [-1, 32]             288\n",
      "            Linear-2                   [-1, 32]           1,056\n",
      "              Tanh-3                   [-1, 32]               0\n",
      "            Linear-4                   [-1, 32]             288\n",
      "            Linear-5                   [-1, 32]           1,056\n",
      "              Tanh-6                   [-1, 32]               0\n",
      "            Linear-7                   [-1, 32]             288\n",
      "            Linear-8                   [-1, 32]           1,056\n",
      "              Tanh-9                   [-1, 32]               0\n",
      "           Linear-10                   [-1, 32]             288\n",
      "           Linear-11                   [-1, 32]           1,056\n",
      "             Tanh-12                   [-1, 32]               0\n",
      "           Linear-13                   [-1, 32]             288\n",
      "           Linear-14                   [-1, 32]           1,056\n",
      "             Tanh-15                   [-1, 32]               0\n",
      "           Linear-16                   [-1, 32]             288\n",
      "           Linear-17                   [-1, 32]           1,056\n",
      "             Tanh-18                   [-1, 32]               0\n",
      "           Linear-19                   [-1, 32]             288\n",
      "           Linear-20                   [-1, 32]           1,056\n",
      "             Tanh-21                   [-1, 32]               0\n",
      "           Linear-22                   [-1, 32]             288\n",
      "           Linear-23                   [-1, 32]           1,056\n",
      "             Tanh-24                   [-1, 32]               0\n",
      "           Linear-25                   [-1, 32]             288\n",
      "           Linear-26                   [-1, 32]           1,056\n",
      "             Tanh-27                   [-1, 32]               0\n",
      "           Linear-28                   [-1, 32]             288\n",
      "           Linear-29                   [-1, 32]           1,056\n",
      "             Tanh-30                   [-1, 32]               0\n",
      "           Linear-31                   [-1, 32]             288\n",
      "           Linear-32                   [-1, 32]           1,056\n",
      "             Tanh-33                   [-1, 32]               0\n",
      "           Linear-34                   [-1, 32]             288\n",
      "           Linear-35                   [-1, 32]           1,056\n",
      "             Tanh-36                   [-1, 32]               0\n",
      "           Linear-37                   [-1, 32]             288\n",
      "           Linear-38                   [-1, 32]           1,056\n",
      "             Tanh-39                   [-1, 32]               0\n",
      "           Linear-40                   [-1, 32]             288\n",
      "           Linear-41                   [-1, 32]           1,056\n",
      "             Tanh-42                   [-1, 32]               0\n",
      "           Linear-43                   [-1, 32]             288\n",
      "           Linear-44                   [-1, 32]           1,056\n",
      "             Tanh-45                   [-1, 32]               0\n",
      "           Linear-46                   [-1, 32]             288\n",
      "           Linear-47                   [-1, 32]           1,056\n",
      "             Tanh-48                   [-1, 32]               0\n",
      "           Linear-49                   [-1, 32]             288\n",
      "           Linear-50                   [-1, 32]           1,056\n",
      "             Tanh-51                   [-1, 32]               0\n",
      "           Linear-52                   [-1, 32]             288\n",
      "           Linear-53                   [-1, 32]           1,056\n",
      "             Tanh-54                   [-1, 32]               0\n",
      "           Linear-55                   [-1, 32]             288\n",
      "           Linear-56                   [-1, 32]           1,056\n",
      "             Tanh-57                   [-1, 32]               0\n",
      "           Linear-58                   [-1, 32]             288\n",
      "           Linear-59                   [-1, 32]           1,056\n",
      "             Tanh-60                   [-1, 32]               0\n",
      "           Linear-61                   [-1, 32]             288\n",
      "           Linear-62                   [-1, 32]           1,056\n",
      "             Tanh-63                   [-1, 32]               0\n",
      "           Linear-64                   [-1, 32]             288\n",
      "           Linear-65                   [-1, 32]           1,056\n",
      "             Tanh-66                   [-1, 32]               0\n",
      "           Linear-67                   [-1, 32]             288\n",
      "           Linear-68                   [-1, 32]           1,056\n",
      "             Tanh-69                   [-1, 32]               0\n",
      "           Linear-70                   [-1, 32]             288\n",
      "           Linear-71                   [-1, 32]           1,056\n",
      "             Tanh-72                   [-1, 32]               0\n",
      "           Linear-73                   [-1, 32]             288\n",
      "           Linear-74                   [-1, 32]           1,056\n",
      "             Tanh-75                   [-1, 32]               0\n",
      "           Linear-76                   [-1, 32]             288\n",
      "           Linear-77                   [-1, 32]           1,056\n",
      "             Tanh-78                   [-1, 32]               0\n",
      "           Linear-79                   [-1, 32]             288\n",
      "           Linear-80                   [-1, 32]           1,056\n",
      "             Tanh-81                   [-1, 32]               0\n",
      "           Linear-82                   [-1, 32]             288\n",
      "           Linear-83                   [-1, 32]           1,056\n",
      "             Tanh-84                   [-1, 32]               0\n",
      "           Linear-85                   [-1, 32]             288\n",
      "           Linear-86                   [-1, 32]           1,056\n",
      "             Tanh-87                   [-1, 32]               0\n",
      "           Linear-88                   [-1, 32]             288\n",
      "           Linear-89                   [-1, 32]           1,056\n",
      "             Tanh-90                   [-1, 32]               0\n",
      "           Linear-91                   [-1, 32]             288\n",
      "           Linear-92                   [-1, 32]           1,056\n",
      "             Tanh-93                   [-1, 32]               0\n",
      "           Linear-94                   [-1, 32]             288\n",
      "           Linear-95                   [-1, 32]           1,056\n",
      "             Tanh-96                   [-1, 32]               0\n",
      "           Linear-97                    [-1, 3]              99\n",
      "================================================================\n",
      "Total params: 43,107\n",
      "Trainable params: 43,107\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.16\n",
      "Estimated Total Size (MB): 0.19\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "input_dim = 8\n",
    "hidden_size = 32\n",
    "model = RNN(input_dim, hidden_size, 3)\n",
    "summary(model, input_size=(32, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cbcbc74-267f-44d4-b086-bcc0fadc3311",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s_/w_s3ltrx3_v1172gl5zh3r700000gn/T/ipykernel_82619/3933072840.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.labels = torch.tensor((sums % num_classes).long())  # Convert to 3-class classification\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Loss: 1.1011\n",
      "Epoch [11/200], Loss: 1.0755\n",
      "Epoch [21/200], Loss: 1.0574\n",
      "Epoch [31/200], Loss: 1.0450\n",
      "Epoch [41/200], Loss: 1.0231\n",
      "Epoch [51/200], Loss: 1.0104\n",
      "Epoch [61/200], Loss: 0.9973\n",
      "Epoch [71/200], Loss: 0.9841\n",
      "Epoch [81/200], Loss: 0.9396\n",
      "Epoch [91/200], Loss: 0.9138\n",
      "Epoch [101/200], Loss: 0.8781\n",
      "Epoch [111/200], Loss: 0.8280\n",
      "Epoch [121/200], Loss: 0.8083\n",
      "Epoch [131/200], Loss: 0.7351\n",
      "Epoch [141/200], Loss: 0.6787\n",
      "Epoch [151/200], Loss: 0.6374\n",
      "Epoch [161/200], Loss: 0.6173\n",
      "Epoch [171/200], Loss: 0.5722\n",
      "Epoch [181/200], Loss: 0.5138\n",
      "Epoch [191/200], Loss: 0.4768\n",
      "Sample Predictions: [1 2 0 0 2 1 2 1 0 0 1 1 2 2 1 0]\n",
      "Actual Labels:      [1 2 0 1 2 0 2 1 0 0 1 1 1 2 1 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define Toy Dataset\n",
    "class ToySequenceDataset(Dataset):\n",
    "    def __init__(self, num_samples=500, sequence_length=10, input_dim=5, num_classes=3):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Generate random sequences\n",
    "        self.data = torch.rand(num_samples, sequence_length, input_dim)  # Shape: (num_samples, seq_length, input_dim)\n",
    "        \n",
    "        # Labels based on sum over sequence\n",
    "        sums = self.data.sum(dim=(1, 2))  # Sum across sequence and features\n",
    "        self.labels = torch.tensor((sums % num_classes).long())  # Convert to 3-class classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# Initialize Dataset and DataLoader\n",
    "batch_size = 16\n",
    "toy_dataset = ToySequenceDataset(num_samples=500)\n",
    "train_loader = DataLoader(toy_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model Configuration\n",
    "input_size = 5  # Features per timestep\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = 3  # Classification into 3 categories\n",
    "learning_rate = 0.001\n",
    "num_epochs = 200\n",
    "\n",
    "# Training Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = RNN(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sequences, labels in train_loader:\n",
    "        sequences, labels = sequences.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Test Model on Sample Data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_sequences, test_labels = next(iter(train_loader))  # Get a small batch\n",
    "    test_sequences, test_labels = test_sequences.to(device), test_labels.to(device)\n",
    "\n",
    "    predictions = model(test_sequences)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "    print(\"Sample Predictions:\", predicted_classes.cpu().numpy())\n",
    "    print(\"Actual Labels:     \", test_labels.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3185d39-348d-45e5-beea-eb2ed94a7465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
