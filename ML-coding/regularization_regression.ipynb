{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_X = iris.data\n",
    "iris_Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_l2 = SGDRegressor(loss='squared_loss', penalty='l2', alpha=0.0001, fit_intercept=True, max_iter=30000)\n",
    "lr_l2.fit(iris_X, iris_Y)\n",
    "prediction_sk = lr_l2.predict(iris_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.11208222, -0.04034415,  0.22978946,  0.60880311]),\n",
       " array([0.18918036]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_l2.coef_, lr_l2.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression:\n",
    "    def __init__(self, fit_intercept=False,learning_rate=0.001,n_iterations=1000\n",
    "                 ,closed_formed=False, regularization=None):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "        \n",
    "        self.closed_form = closed_formed\n",
    "        if not regularization:\n",
    "            self.regularization = lambda x: 0\n",
    "            self.regularization.grad = lambda x: 0 \n",
    "        \n",
    "    def initialize_weight(self, X):\n",
    "        n_features = X.shape[1]\n",
    "        limit = 1/math.sqrt(n_features)\n",
    "        weights = np.random.uniform(-limit,limit,(n_features,))\n",
    "        return weights\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.insert(iris_X, 0, 1, axis=1)\n",
    "        weight = self.initialize_weight(X)\n",
    "        \n",
    "        ########### Algebra Way #################\n",
    "        if self.closed_form:\n",
    "            self.coef_ = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "            \n",
    "        ########## Gradient Descent #############\n",
    "        else:\n",
    "            min_mse = float(\"inf\")\n",
    "            cur_mse = float(\"inf\")\n",
    "            n_samples = X.shape[0]\n",
    "            for i in range(self.n_iterations):\n",
    "                y_pred = X.dot(weight)\n",
    "                residual =  -(1/n_samples) * (Y-y_pred) \n",
    "                gradient = residual.dot(X) + self.regularization.grad(weight)\n",
    "                weight -= self.learning_rate*(gradient)\n",
    "\n",
    "                # Calculate l2 loss\n",
    "                mse = np.mean(0.5 * (Y - y_pred)**2 + self.regularization(weight))\n",
    "                if mse > cur_mse:\n",
    "                    break\n",
    "                cur_mse = mse\n",
    "        \n",
    "            self.coef_ = weight\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = np.insert(iris_X, 0, 1, axis=1)\n",
    "        prediction = X.dot(self.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13173604 -0.10388661 -0.03522212  0.22774142  0.60595097]\n",
      "[ 0.18649525 -0.11190585 -0.04007949  0.22864503  0.60925205]\n"
     ]
    }
   ],
   "source": [
    "mylr = MyLinearRegression(fit_intercept=True,learning_rate=0.01,n_iterations=30000)\n",
    "mylr.fit(iris_X, iris_Y)\n",
    "print(mylr.coef_)\n",
    "\n",
    "mylr2 = MyLinearRegression(fit_intercept=True,closed_formed=True)\n",
    "mylr2.fit(iris_X, iris_Y)\n",
    "print(mylr2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class l2_regularization():\n",
    "    \"\"\" Regularization for Lasso Regression \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, w):\n",
    "        return 0.5*self.alpha*w.T.dot(w)\n",
    "\n",
    "    def grad(self, w):\n",
    "        return self.alpha*w\n",
    "\n",
    "class MyRidgeRegression(MyLinearRegression):\n",
    "    \n",
    "    def __init__(self, alpha=0.0001, fit_intercept=False,learning_rate=0.01,n_iterations=30000):\n",
    "        self.regularization = l2_regularization(alpha)\n",
    "        super().__init__(fit_intercept,learning_rate,n_iterations,regularization=self.regularization)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14584957 -0.10623369 -0.03626178  0.22869961  0.60535734]\n"
     ]
    }
   ],
   "source": [
    "mylr_l2 = MyRidgeRegression(fit_intercept=True)\n",
    "mylr_l2.fit(iris_X, iris_Y)\n",
    "print(mylr_l2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### Utilities ########################################\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def normalize(X, axis=-1, order=2):\n",
    "    \"\"\" Normalize the dataset X \"\"\"\n",
    "    l2 = np.linalg.norm(X, order, axis)\n",
    "    return X/np.expand_dims(l2, axis)\n",
    "\n",
    "\n",
    "def polynomial_features(X, degree=2):\n",
    "    n_samples, n_features = np.shape(X)\n",
    "\n",
    "    def index_combinations():\n",
    "        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n",
    "        flat_combs = [item for sublist in combs for item in sublist]\n",
    "        return flat_combs\n",
    "    \n",
    "    combinations = index_combinations()\n",
    "    n_output_features = len(combinations)\n",
    "    X_new = np.empty((n_samples, n_output_features))\n",
    "    \n",
    "    for i, index_combs in enumerate(combinations):  \n",
    "        X_new[:, i] = np.prod(X[:, index_combs], axis=1)\n",
    "\n",
    "    return X_new\n",
    "\n",
    "def soft_threshold(gradient,alpha):\n",
    "    '''Soft threshold function used for normalized data and lasso regression'''\n",
    "    if gradient < 0.0 and  alpha< abs(gradient):\n",
    "        return gradient + alpha\n",
    "    elif gradient > 0.0 and alpha < abs(gradient):\n",
    "        return gradient - alpha\n",
    "    else: \n",
    "        return 0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6179.7300000000005"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_threshold(6329.7300000000005,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_X[:1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "class l1_regularization(MyLinearRegression):\n",
    "    \"\"\" Regularization for Lasso Regression \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, w):\n",
    "        return self.alpha*np.linalg.norm(w, ord=1)\n",
    "\n",
    "    def grad(self, w):\n",
    "        return self.alpha*np.sign(w)\n",
    "\n",
    "class MyLassoRegression(MyLinearRegression):\n",
    "    \n",
    "    def __init__(self, alpha=1, fit_intercept=False,learning_rate=0.01,n_iterations=1000):\n",
    "        \n",
    "        self.regularization = l1_regularization(alpha)\n",
    "        super().__init__(fit_intercept,learning_rate,n_iterations,regularization=self.regularization)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.insert(iris_X, 0, 1, axis=1)\n",
    "       #X = normalize(X)\n",
    "        col_num = X.shape[1]\n",
    "        weight = np.zeros(col_num)\n",
    "        \n",
    "        \n",
    "        for iteration in range(self.n_iterations):\n",
    "            start = 1 if self.fit_intercept else 0\n",
    "            for j in range(start, col_num):\n",
    "                X_j = X[:,j]\n",
    "                \n",
    "                tmp_weight = weight.copy()\n",
    "                tmp_weight[j] = 0.0\n",
    "                \n",
    "                y_pred = X.dot(tmp_weight)\n",
    "                gradient = np.dot(X_j, y-y_pred)\n",
    "                alpha = self.regularization.alpha*X.shape[0]\n",
    "                \n",
    "                weight[j] = soft_threshold(gradient, alpha)/(X[:, j]**2).sum()\n",
    "                \n",
    "            if self.fit_intercept:\n",
    "                weight[0] = np.sum(y - np.dot(X[:, 1:], weight[1:]))/(X.shape[0])\n",
    "\n",
    "            self.coef_ = weight\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = normalize(X, order=2)\n",
    "        prediction = super().predict(X)\n",
    "        return prediction\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.55890632 0.         0.         0.11737458 0.        ]\n"
     ]
    }
   ],
   "source": [
    "mylr_l1 = MyLassoRegression(fit_intercept=True, alpha=1, learning_rate=0.01,n_iterations=1000)\n",
    "mylr_l1.fit(iris_X, iris_Y)\n",
    "print(mylr_l1.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr_l1 = SGDRegressor(loss='squared_loss', penalty='l1', alpha=1, \n",
    "                     fit_intercept=True, max_iter=1000, eta0=0.01, learning_rate=\"constant\")\n",
    "lr_l1.fit(iris_X, iris_Y)\n",
    "prediction_sk = lr_l1.predict(iris_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.        , 0.08556442, 0.        ]), array([0.5507914]))"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_l1.coef_, lr_l1.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/satopirka/Lasso/blob/master/lasso.py\n",
    "# https://courses.cs.washington.edu/courses/cse446/17wi/slides/lasso-annotated.pdf\n",
    "# https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/supervised_learning/regression.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
