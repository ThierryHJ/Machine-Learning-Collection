{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_diabetes\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data iris\n",
    "iris = load_iris()\n",
    "iris_X = iris.data\n",
    "iris_Y = iris.target\n",
    "\n",
    "\n",
    "# Load the data diabetes\n",
    "diabetes = load_diabetes()\n",
    "diabetes_X = diabetes.data\n",
    "diabetes_Y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the data Titanic\n",
    "titanic = pd.read_csv('./data/titanic/train.csv')\n",
    "titanic_test = pd.read_csv('./data/titanic/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_Y_df=titanic.loc[:, titanic.columns.isin(['Survived'])]\n",
    "titanic_X=titanic.loc[:, ~titanic.columns.isin(['Survived',\"Name\",\"Ticket\",\"Cabin\",\"PassengerId\",\"Embarked\"])]\n",
    "titanic_X_test=titanic_test.loc[:, ~titanic_test.columns.isin(['Survived',\"Name\",\"Ticket\",\"Cabin\",\"PassengerId\",\"Embarked\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(df, col):\n",
    "    new_col = pd.get_dummies(df[col], prefix=col)\n",
    "    df = pd.concat([df,new_col], axis=1).drop([col], axis=1)\n",
    "    return df\n",
    "\n",
    "def fill_na_mean(df, col):\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "titanic_X = one_hot_encoding(titanic_X,\"Sex\")\n",
    "titanic_X_df = one_hot_encoding(titanic_X,\"Pclass\")\n",
    "for col in titanic_X_df:\n",
    "    fill_na_mean(titanic_X_df, col)\n",
    "\n",
    "titanic_X_test = one_hot_encoding(titanic_X_test,\"Sex\")\n",
    "titanic_X_test_df = one_hot_encoding(titanic_X_test,\"Pclass\")\n",
    "for col in titanic_X_test_df:\n",
    "    fill_na_mean(titanic_X_test_df, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, False, False, False]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[titanic_X_df[col].isnull().values.any() for col in titanic_X_df]\n",
    "[titanic_X_test_df[col].isnull().values.any() for col in titanic_X_test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y=np.array([i for i in Y])\n",
    "titanic_X=titanic_X_df.to_numpy()\n",
    "titanic_X_test = titanic_X_test_df.to_numpy()\n",
    "titanic_Y=titanic_Y_df.to_numpy().reshape(len(titanic_Y_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22.    ,  1.    ,  0.    ,  7.25  ,  0.    ,  1.    ,  0.    ,\n",
       "         0.    ,  1.    ],\n",
       "       [38.    ,  1.    ,  0.    , 71.2833,  1.    ,  0.    ,  1.    ,\n",
       "         0.    ,  0.    ],\n",
       "       [26.    ,  0.    ,  0.    ,  7.925 ,  1.    ,  0.    ,  0.    ,\n",
       "         0.    ,  1.    ],\n",
       "       [35.    ,  1.    ,  0.    , 53.1   ,  1.    ,  0.    ,  1.    ,\n",
       "         0.    ,  0.    ],\n",
       "       [35.    ,  0.    ,  0.    ,  8.05  ,  0.    ,  1.    ,  0.    ,\n",
       "         0.    ,  1.    ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Pclass_1',\n",
       "        'Pclass_2', 'Pclass_3'],\n",
       "       dtype='object'),\n",
       " Index(['Age', 'SibSp', 'Parch', 'Fare', 'Sex_female', 'Sex_male', 'Pclass_1',\n",
       "        'Pclass_2', 'Pclass_3'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_X_df.columns, titanic_X_test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utilities\n",
    "def cal_gini(class_values, splits):\n",
    "    '''calculate the gini index based on classes and data splits'''\n",
    "    gini = 0.0\n",
    "    total_size = 0\n",
    "    \n",
    "    for split in splits:\n",
    "        size = len(split)\n",
    "        if size == 0: continue\n",
    "        for class_value in class_values:\n",
    "            proportion = [row[-1] for row in split].count(class_value)/ float(size)\n",
    "            gini += (proportion*(1.0-proportion))*size\n",
    "        total_size += size\n",
    "\n",
    "    return gini/total_size\n",
    "\n",
    "\n",
    "def cal_variance(class_values, splits):\n",
    "    total_variance = 0.0\n",
    "    total_size = 0.0\n",
    "    \n",
    "    for split in splits:\n",
    "        size = len(split)\n",
    "        split_mean = np.mean([row[-1] for row in split])\n",
    "        variance = sum([(row[-1]-split_mean)**2 for row in split])/ float(size-1)\n",
    "        total_variance +=  variance*size\n",
    "        total_size += size\n",
    "    \n",
    "    return total_variance/float(total_size)\n",
    "\n",
    "def test_split(col_index, threshold, datasets):\n",
    "    '''list all the potential splits for each col'''\n",
    "    left, right = [], []\n",
    "    \n",
    "    for row in datasets:\n",
    "        if row[col_index] < threshold: left.append(row)\n",
    "        else: right.append(row)\n",
    "            \n",
    "    return left, right\n",
    "\n",
    "def get_split(datasets, criterion=cal_gini):\n",
    "    '''find the best split'''\n",
    "    class_values = list(set(list([row[-1] for row in datasets])))\n",
    "    best_col, best_threshold, best_score, best_group = float(\"inf\"), float(\"inf\"), float(\"inf\"), None\n",
    "    \n",
    "    for col in range(len(datasets[0])-1):\n",
    "        for row in range(len(datasets)):\n",
    "            group = test_split(col, datasets[row][col], datasets)\n",
    "            # use gini index to measure impurity here\n",
    "            node_index = criterion(class_values, group)\n",
    "            if node_index < best_score:\n",
    "                best_col, best_threshold, best_score, best_group = \\\n",
    "                col, datasets[row][col], node_index, group\n",
    "                \n",
    "    return {'col_index': best_col, 'threshold': best_threshold, 'groups': best_group}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[X2 < 3.163]\n",
      " [X1 < 2.999]\n",
      "  [X1 < 1.729]\n",
      "   [0.3]\n",
      "   [0.3]\n",
      "  [X2 < 2.620]\n",
      "   [X1 < 2.999]\n",
      "    [0.9]\n",
      "    [0.9]\n",
      "   [X1 < 3.678]\n",
      "    [0.7]\n",
      "    [0.7]\n",
      " [X1 < 9.002]\n",
      "  [X1 < 6.642]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "  [X1 < 9.002]\n",
      "   [1.3]\n",
      "   [1.3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "#### Testing ################\n",
    "# Create child splits for a node or make terminal\n",
    "def split(node, max_depth, min_size, depth):\n",
    "    left, right = node['groups']\n",
    "    del (node['groups'])\n",
    "    # check for a no split\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    # check for max depth\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "        return\n",
    "    # process left child\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left, criterion=cal_variance)\n",
    "        split(node['left'], max_depth, min_size, depth + 1)\n",
    "    # process right child\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right, criterion=cal_variance)\n",
    "        split(node['right'], max_depth, min_size, depth + 1)\n",
    "\n",
    "# Create a terminal node value\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Build a decision tree\n",
    "def build_tree(X, y, max_depth, min_size):\n",
    "    datasets = np.column_stack([X, y.T])\n",
    "    root = get_split(datasets)\n",
    "    split(root, max_depth, min_size, 1)\n",
    "    return root\n",
    "\n",
    "\n",
    "# Print a decision tree\n",
    "def print_tree(node, depth=0):\n",
    "    if isinstance(node, dict):\n",
    "        print('%s[X%d < %.3f]' % ((depth * ' ', (node['col_index'] + 1), node['threshold'])))\n",
    "        print_tree(node['left'], depth + 1)\n",
    "        print_tree(node['right'], depth + 1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((depth * ' ', node)))\n",
    "\n",
    "d1 = [[2.771244718, 1.784783929, 0.3],\n",
    "   [1.728571309, 1.169761413, 0.6],\n",
    "   [3.678319846, 2.81281357, 0.7],\n",
    "   [3.961043357, 2.61995032, 0.7],\n",
    "   [2.999208922, 2.209014212, 0.9],\n",
    "   [7.497545867, 3.162953546, 1],\n",
    "   [9.00220326, 3.339047188, 1.3],\n",
    "   [7.444542326, 0.476683375, 1.1],\n",
    "   [10.12493903, 3.234550982, 1],\n",
    "   [6.642287351, 3.319983761, 1]]\n",
    "d1 = np.array(d1)\n",
    "d1_X = d1[:,:2]\n",
    "d1_Y = d1[:,-1]\n",
    "tree = build_tree(d1_X, d1_Y, 5, 1)\n",
    "print_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTree:\n",
    "    \n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None, impurity_calculation=None):\n",
    "        self.tree = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        self._impurity_calculation = impurity_calculation\n",
    "        # Function to determine prediction of y at leaf\n",
    "        self._leaf_value_calculation = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.tree = self._build_tree(X, y)\n",
    "    \n",
    "    def _build_tree(self, X, y):\n",
    "        datasets = np.column_stack([X, y])\n",
    "        root = get_split(datasets, criterion=self._impurity_calculation)\n",
    "        split(root, self.max_depth, self.min_samples_split, 1)\n",
    "        return root\n",
    "        \n",
    "    def predict_point(self, row, node=None):\n",
    "        if not node: node = self.tree\n",
    "        \n",
    "        if row[node[\"col_index\"]] < node[\"threshold\"]:\n",
    "            if isinstance(node[\"left\"], dict):\n",
    "                return self.predict_point(row, node[\"left\"])\n",
    "            else:\n",
    "                return node[\"left\"]\n",
    "        else:\n",
    "            if isinstance(node[\"right\"], dict):\n",
    "                return self.predict_point(row, node[\"right\"])\n",
    "            else:\n",
    "                return node[\"right\"]\n",
    "    \n",
    "    def predict(self, X_test, node=None):\n",
    "        if not node: node = self.tree\n",
    "        prediction = []\n",
    "        \n",
    "        for row in X_test:\n",
    "            prediction.append(self.predict_point(row,self.tree))\n",
    "        \n",
    "        return np.array(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeClassifier(MyDecisionTree):\n",
    "    \n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None, impurity_calculation=cal_gini):\n",
    "\n",
    "        super().__init__(min_samples_split, min_impurity,\n",
    "                 max_depth, impurity_calculation=impurity_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDecisionTreeRegressor(MyDecisionTree):\n",
    "    \n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None, impurity_calculation=cal_variance):\n",
    "\n",
    "        super().__init__(min_samples_split, min_impurity,\n",
    "                 max_depth, impurity_calculation=impurity_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "###################### Test Case #######################\n",
    "d1 = [[2.771244718, 1.784783929, 0],\n",
    "   [1.728571309, 1.169761413, 0],\n",
    "   [3.678319846, 2.81281357, 0],\n",
    "   [3.961043357, 2.61995032, 0],\n",
    "   [2.999208922, 2.209014212, 0],\n",
    "   [7.497545867, 3.162953546, 1],\n",
    "   [9.00220326, 3.339047188, 1],\n",
    "   [7.444542326, 0.476683375, 1],\n",
    "   [10.12493903, 3.234550982, 1],\n",
    "   [6.642287351, 3.319983761, 1]]\n",
    "d1 = np.array(d1)\n",
    "d1_X = d1[:,:2]\n",
    "d1_Y = d1[:,-1]\n",
    "d1_X_test = d1[:-2,:2]\n",
    "\n",
    "d_clf = MyDecisionTreeClassifier(max_depth=5, min_samples_split=1)\n",
    "d_clf.fit(d1_X,d1_Y)\n",
    "prediction = d_clf.predict(d1_X_test)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0.\n",
      " 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1.\n",
      " 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 1. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "d_clf = MyDecisionTreeClassifier(max_depth=5, min_samples_split=2)\n",
    "d_clf.fit(titanic_X,titanic_Y)\n",
    "d_prediction = d_clf.predict(titanic_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[265.  72. 122. 131.  53.  72.  53.  66. 129. 122.  53. 129.  72. 131.\n",
      "  72. 131. 265. 131.  53.  72.  53.  53.  72. 230. 129. 131.  72. 129.\n",
      " 109. 292. 129.  72. 321.  72.  53. 170. 265. 131. 233. 131. 109.  53.\n",
      "  53.  72. 122.  53.  53.  53.  66. 131.  53. 265.  72.  53. 129.  72.\n",
      " 122.  72. 170. 170.  72. 131.  65.  53.  53. 170. 129.  53.  53.  53.\n",
      "  72. 265. 131.  53. 109. 170. 191.  53. 252.  53. 131.  53.  72. 136.\n",
      "  72. 129.  70. 131.  65.  72.  72. 122. 109.  72.  72. 170. 131. 264.\n",
      "  53. 170. 122.  66. 302. 288. 129.  53.  53. 129. 265.  53.  66.  72.\n",
      " 302. 292. 275. 131. 265. 292. 292. 170.  53. 265. 233. 122. 129. 131.\n",
      "  72. 109.  72. 268. 321.  53.  53.  53. 129. 122.  72. 265. 321. 265.\n",
      " 191. 268. 131.  72. 122. 288. 122. 233. 129. 131. 288.  72. 292.  53.\n",
      " 122. 265.  53. 170.  53. 265.  53. 230. 129. 288. 131.  72.  70. 275.\n",
      " 268. 170.  66.  53. 275.  53. 131.  53. 265. 122.  72. 109. 131.  72.\n",
      " 131. 168. 122. 131. 122.  72. 109.  53. 131. 178.  66. 131. 170. 122.\n",
      "  72. 129. 136. 170.  72.  65. 131. 265. 129. 122. 122. 265. 265.  53.\n",
      " 191. 191. 109.  72.  72. 275. 265. 265. 170. 252.  70. 170. 252.  53.\n",
      "  53. 265.  65.  66.  53.  65. 264. 170. 131. 129. 265.  53. 131.  72.\n",
      " 129. 122. 275.  53.  70.  53. 136.  72. 170.  72. 265. 265. 230. 233.\n",
      " 129. 122. 230. 170. 321.  65.  53.  53.  72.  72. 275.  53. 129. 170.\n",
      "  65. 131. 264.  72. 265. 191. 136. 292. 170. 109. 265.  72. 170.  53.\n",
      " 265. 170. 253.  53. 252. 131.  72. 131.  66. 122. 321. 265.  72. 288.\n",
      "  53. 122.  72.  53.  53.  53. 275.  65. 265. 265. 253. 170.  53. 168.\n",
      "  72. 292. 288. 122.  53. 265. 129.  72. 292. 168. 109. 292. 122. 230.\n",
      " 230. 292. 131. 122. 131. 233. 122.  53. 191.  53. 275. 131.  53.  72.\n",
      " 264. 109.  53. 131.  53. 288. 191. 191. 109. 109. 265. 170.  72.  53.\n",
      " 265.  53.  66. 170. 230.  53.  53. 265.  72. 292. 265.  72. 321. 170.\n",
      " 292. 168. 233. 233. 292. 129.  72. 131.  65.  53. 178. 131. 122. 129.\n",
      " 131.  72. 191.  53. 122.  53. 170.  53.  72. 136. 170.  65. 275.  70.\n",
      "  53.  53. 230. 131.  70. 131. 122. 265. 288.  72. 168. 275. 288. 275.\n",
      "  72. 129. 170. 131. 170. 136. 321.  72. 131. 131. 288. 129.  72.  53.\n",
      "  53. 265. 233. 170. 131.  72. 168.  53. 275.  72.  53.  66. 122.  72.\n",
      "  72.  53.  72. 265.  53.  53. 122.  72.]\n"
     ]
    }
   ],
   "source": [
    "d_reg = MyDecisionTreeRegressor(max_depth=5, min_samples_split=2)\n",
    "d_reg.fit(diabetes_X,diabetes_Y)\n",
    "d_prediction_reg = d_reg.predict(diabetes_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0 1 0 1 1 1 0 0 0 1 0 1 0 0\n",
      " 0 0 1 0 0 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 0 0 1 0\n",
      " 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1\n",
      " 0 1 1 0 0 0 0 1 0 1 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 0\n",
      " 1 1 1 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1 0\n",
      " 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 0 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 1 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "############ The results from sklearn official API ###########################\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(titanic_X,titanic_Y)\n",
    "prediction = clf.predict(titanic_X_test)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8253588516746412"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([d_prediction[i]==prediction[i] for i in range(len(prediction))])/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.229329494035476\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "reg = DecisionTreeRegressor(random_state=0, max_depth=5)\n",
    "reg.fit(diabetes_X,diabetes_Y)\n",
    "prediction_reg = reg.predict(diabetes_X)\n",
    "print(sum([abs(prediction_reg[i]-d_prediction_reg[i]) for i in range(len(prediction_reg))])/len(prediction_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################### More Test Cases ############################################\n",
    "import math \n",
    "\n",
    "def divide_on_feature(X, feature_i, threshold):\n",
    "    \"\"\" Divide dataset based on if sample value on feature index is larger than\n",
    "        the given threshold \"\"\"\n",
    "    split_func = None\n",
    "    if isinstance(threshold, int) or isinstance(threshold, float):\n",
    "        split_func = lambda sample: sample[feature_i] >= threshold\n",
    "    else:\n",
    "        split_func = lambda sample: sample[feature_i] == threshold\n",
    "\n",
    "    X_1 = np.array([sample for sample in X if split_func(sample)])\n",
    "    X_2 = np.array([sample for sample in X if not split_func(sample)])\n",
    "\n",
    "    return np.array([X_1, X_2])\n",
    "\n",
    "def calculate_entropy(y):\n",
    "    \"\"\" Calculate the entropy of label array y \"\"\"\n",
    "    log2 = lambda x: math.log(x) / math.log(2)\n",
    "    unique_labels = np.unique(y)\n",
    "    entropy = 0\n",
    "    for label in unique_labels:\n",
    "        count = len(y[y == label])\n",
    "        p = count / len(y)\n",
    "        entropy += -p * log2(p)\n",
    "    return entropy\n",
    "\n",
    "class DecisionNode():\n",
    "    \"\"\"Class that represents a decision node or leaf in the decision tree\n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_i: int\n",
    "        Feature index which we want to use as the threshold measure.\n",
    "    threshold: float\n",
    "        The value that we will compare feature values at feature_i against to\n",
    "        determine the prediction.\n",
    "    value: float\n",
    "        The class prediction if classification tree, or float value if regression tree.\n",
    "    true_branch: DecisionNode\n",
    "        Next decision node for samples where features value met the threshold.\n",
    "    false_branch: DecisionNode\n",
    "        Next decision node for samples where features value did not meet the threshold.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_i=None, threshold=None,\n",
    "                 value=None, true_branch=None, false_branch=None):\n",
    "        self.feature_i = feature_i          # Index for the feature that is tested\n",
    "        self.threshold = threshold          # Threshold value for feature\n",
    "        self.value = value                  # Value if the node is a leaf in the tree\n",
    "        self.true_branch = true_branch      # 'Left' subtree\n",
    "        self.false_branch = false_branch    # 'Right' subtree\n",
    "\n",
    "\n",
    "# Super class of RegressionTree and ClassificationTree\n",
    "class DecisionTree(object):\n",
    "    \"\"\"Super class of RegressionTree and ClassificationTree.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    min_samples_split: int\n",
    "        The minimum number of samples needed to make a split when building a tree.\n",
    "    min_impurity: float\n",
    "        The minimum impurity required to split the tree further.\n",
    "    max_depth: int\n",
    "        The maximum depth of a tree.\n",
    "    loss: function\n",
    "        Loss function that is used for Gradient Boosting models to calculate impurity.\n",
    "    \"\"\"\n",
    "    def __init__(self, min_samples_split=2, min_impurity=1e-7,\n",
    "                 max_depth=float(\"inf\"), loss=None):\n",
    "        self.root = None  # Root node in dec. tree\n",
    "        # Minimum n of samples to justify split\n",
    "        self.min_samples_split = min_samples_split\n",
    "        # The minimum impurity to justify split\n",
    "        self.min_impurity = min_impurity\n",
    "        # The maximum depth to grow the tree to\n",
    "        self.max_depth = max_depth\n",
    "        # Function to calculate impurity (classif.=>info gain, regr=>variance reduct.)\n",
    "        self._impurity_calculation = None\n",
    "        # Function to determine prediction of y at leaf\n",
    "        self._leaf_value_calculation = None\n",
    "        # If y is one-hot encoded (multi-dim) or not (one-dim)\n",
    "        self.one_dim = None\n",
    "        # If Gradient Boost\n",
    "        self.loss = loss\n",
    "\n",
    "    def fit(self, X, y, loss=None):\n",
    "        \"\"\" Build decision tree \"\"\"\n",
    "        self.one_dim = len(np.shape(y)) == 1\n",
    "        self.root = self._build_tree(X, y)\n",
    "        self.loss=None\n",
    "\n",
    "    def _build_tree(self, X, y, current_depth=0):\n",
    "        \"\"\" Recursive method which builds out the decision tree and splits X and respective y\n",
    "        on the feature of X which (based on impurity) best separates the data\"\"\"\n",
    "\n",
    "        largest_impurity = 0\n",
    "        best_criteria = None    # Feature index and threshold\n",
    "        best_sets = None        # Subsets of the data\n",
    "\n",
    "        # Check if expansion of y is needed\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        n_samples, n_features = np.shape(X)\n",
    "\n",
    "        if n_samples >= self.min_samples_split and current_depth <= self.max_depth:\n",
    "            # Calculate the impurity for each feature\n",
    "            for feature_i in range(n_features):\n",
    "                # All values of feature_i\n",
    "                feature_values = np.expand_dims(X[:, feature_i], axis=1)\n",
    "                unique_values = np.unique(feature_values)\n",
    "\n",
    "                # Iterate through all unique values of feature column i and\n",
    "                # calculate the impurity\n",
    "                for threshold in unique_values:\n",
    "                    # Divide X and y depending on if the feature value of X at index feature_i\n",
    "                    # meets the threshold\n",
    "                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)\n",
    "\n",
    "                    if len(Xy1) > 0 and len(Xy2) > 0:\n",
    "                        # Select the y-values of the two sets\n",
    "                        y1 = Xy1[:, n_features:]\n",
    "                        y2 = Xy2[:, n_features:]\n",
    "\n",
    "                        # Calculate impurity\n",
    "                        impurity = self._impurity_calculation(y, y1, y2)\n",
    "\n",
    "                        # If this threshold resulted in a higher information gain than previously\n",
    "                        # recorded save the threshold value and the feature\n",
    "                        # index\n",
    "                        if impurity > largest_impurity:\n",
    "                            largest_impurity = impurity\n",
    "                            best_criteria = {\"feature_i\": feature_i, \"threshold\": threshold}\n",
    "                            best_sets = {\n",
    "                                \"leftX\": Xy1[:, :n_features],   # X of left subtree\n",
    "                                \"lefty\": Xy1[:, n_features:],   # y of left subtree\n",
    "                                \"rightX\": Xy2[:, :n_features],  # X of right subtree\n",
    "                                \"righty\": Xy2[:, n_features:]   # y of right subtree\n",
    "                                }\n",
    "\n",
    "        if largest_impurity > self.min_impurity:\n",
    "            # Build subtrees for the right and left branches\n",
    "            true_branch = self._build_tree(best_sets[\"leftX\"], best_sets[\"lefty\"], current_depth + 1)\n",
    "            false_branch = self._build_tree(best_sets[\"rightX\"], best_sets[\"righty\"], current_depth + 1)\n",
    "            return DecisionNode(feature_i=best_criteria[\"feature_i\"], threshold=best_criteria[\n",
    "                                \"threshold\"], true_branch=true_branch, false_branch=false_branch)\n",
    "\n",
    "        # We're at leaf => determine value\n",
    "        leaf_value = self._leaf_value_calculation(y)\n",
    "\n",
    "        return DecisionNode(value=leaf_value)\n",
    "\n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        feature_value = x[tree.feature_i]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "\n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        \"\"\" Recursively print the decision tree \"\"\"\n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we're at leaf => print the label\n",
    "        if tree.value is not None:\n",
    "            print (tree.value)\n",
    "        # Go deeper down the tree\n",
    "        else:\n",
    "            # Print test\n",
    "            print (\"%s:%s? \" % (tree.feature_i, tree.threshold))\n",
    "            # Print the true scenario\n",
    "            print (\"%sT->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.true_branch, indent + indent)\n",
    "            # Print the false scenario\n",
    "            print (\"%sF->\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.false_branch, indent + indent)\n",
    "\n",
    "\n",
    "class ClassificationTree(DecisionTree):\n",
    "    def _calculate_information_gain(self, y, y1, y2):\n",
    "        # Calculate information gain\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p * \\\n",
    "            calculate_entropy(y1) - (1 - p) * \\\n",
    "            calculate_entropy(y2)\n",
    "\n",
    "        return info_gain\n",
    "\n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self._impurity_calculation = self._calculate_information_gain\n",
    "        self._leaf_value_calculation = self._majority_vote\n",
    "        super(ClassificationTree, self).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "d_clf_1 = ClassificationTree(max_depth=5, min_samples_split=2)\n",
    "d_clf_1.fit(titanic_X,titanic_Y)\n",
    "d_prediction_1 = d_clf_1.predict(titanic_X_test)\n",
    "print(d_prediction_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9784688995215312"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([d_prediction_1[i]==d_prediction[i] for i in range(len(d_prediction))])/len(d_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
