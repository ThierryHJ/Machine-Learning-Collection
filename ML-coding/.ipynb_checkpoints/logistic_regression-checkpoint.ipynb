{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, SGDClassifier, LogisticRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "iris_X = iris.data\n",
    "iris_Y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the data\n",
    "titanic = pd.read_csv('./data/titanic/train.csv')\n",
    "titanic_test = pd.read_csv('./data/titanic/test.csv')\n",
    "\n",
    "titanic_Y_df=titanic.loc[:, titanic.columns.isin(['Survived'])]\n",
    "titanic_X_df=titanic.loc[:, ~titanic.columns.isin(['Survived',\"Name\",\"Ticket\",\"Cabin\",\"PassengerId\",\"Embarked\"])]\n",
    "titanic_X_test_df=titanic_test.loc[:, ~titanic_test.columns.isin(['Survived',\"Name\",\"Ticket\",\"Cabin\",\n",
    "                                                                  \"PassengerId\",\"Embarked\"])]\n",
    "\n",
    "def one_hot_encoding(df, col):\n",
    "    new_col = pd.get_dummies(df[col], prefix=col)\n",
    "    df = pd.concat([df,new_col], axis=1).drop([col], axis=1)\n",
    "    return df\n",
    "\n",
    "def fill_na_mean(df, col):\n",
    "    df[col].fillna(df[col].mean(), inplace=True)\n",
    "\n",
    "titanic_X_df = one_hot_encoding(titanic_X_df,\"Sex\")\n",
    "titanic_X_df = one_hot_encoding(titanic_X_df,\"Pclass\")\n",
    "for col in titanic_X_df:\n",
    "    fill_na_mean(titanic_X_df, col)\n",
    "\n",
    "titanic_X_test_df = one_hot_encoding(titanic_X_test_df,\"Sex\")\n",
    "titanic_X_test_df = one_hot_encoding(titanic_X_test_df,\"Pclass\")\n",
    "for col in titanic_X_test_df:\n",
    "    fill_na_mean(titanic_X_test_df, col)\n",
    "    \n",
    "titanic_X=titanic_X_df.to_numpy()\n",
    "titanic_X_test = titanic_X_test_df.to_numpy()\n",
    "titanic_Y=titanic_Y_df.to_numpy().reshape(len(titanic_Y_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[22.    ,  1.    ,  0.    ,  7.25  ,  0.    ,  1.    ,  0.    ,\n",
       "          0.    ,  1.    ],\n",
       "        [38.    ,  1.    ,  0.    , 71.2833,  1.    ,  0.    ,  1.    ,\n",
       "          0.    ,  0.    ],\n",
       "        [26.    ,  0.    ,  0.    ,  7.925 ,  1.    ,  0.    ,  0.    ,\n",
       "          0.    ,  1.    ],\n",
       "        [35.    ,  1.    ,  0.    , 53.1   ,  1.    ,  0.    ,  1.    ,\n",
       "          0.    ,  0.    ],\n",
       "        [35.    ,  0.    ,  0.    ,  8.05  ,  0.    ,  1.    ,  0.    ,\n",
       "          0.    ,  1.    ]]), array([0, 1, 1, 1, 0]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_X[:5], titanic_Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_diagonal(x):\n",
    "    \"\"\" Converts a vector into an diagonal matrix \"\"\"\n",
    "    m = np.zeros((len(x), len(x)))\n",
    "    for i in range(len(m[0])):\n",
    "        m[i, i] = x[i]\n",
    "    return m\n",
    "\n",
    "def sigmoid(odds):\n",
    "    return 1/(1+np.exp(-odds))\n",
    "        \n",
    "\n",
    "class MyLogisticRegression():\n",
    "    \"\"\" Logistic Regression classifier.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    learning_rate: float\n",
    "        The step length that will be taken when following the negative gradient during\n",
    "        training.\n",
    "    gradient_descent: boolean\n",
    "        True or false depending if gradient descent should be used when training. If\n",
    "        false then we use batch optimization by least squares.\n",
    "    \"\"\" \n",
    "    def __init__(self, fit_intercept=False, learning_rate=.1, n_iteration=1000, gradient_descent=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.param = None\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations=n_iteration\n",
    "        self.intercept_ = None\n",
    "        self.coef_ = None\n",
    "        \n",
    "        self.gradient_descent = gradient_descent\n",
    "\n",
    "    def _initialize_parameters(self, X):\n",
    "        n_features = np.shape(X)[1]\n",
    "        # Initialize parameters between [-1/sqrt(N), 1/sqrt(N)]\n",
    "        weights = np.random.uniform(-1/math.sqrt(n_features), 1/math.sqrt(n_features), (n_features,))\n",
    "        return weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = np.insert(X, 0, 1, axis=1)\n",
    "            \n",
    "        n_samples = X.shape[0]\n",
    "        weights = self._initialize_parameters(X)\n",
    "        print(n_samples, len(y))\n",
    "        # Tune parameters for n iterations\n",
    "        for i in range(self.n_iterations):\n",
    "            # Make a new prediction\n",
    "            y_pred = sigmoid(X.dot(weights))\n",
    "            if self.gradient_descent:\n",
    "                # Move against the gradient of the loss function with\n",
    "                # respect to the parameters to minimize the loss\n",
    "                residual = y-y_pred\n",
    "                gradient = residual.dot(X)*-(1/n_samples)\n",
    "                weights -= self.learning_rate*gradient\n",
    "            #else:\n",
    "                # Make a diagonal matrix of the sigmoid gradient column vector\n",
    "                \n",
    "                # Batch opt:\n",
    "        self.coef_ = weights \n",
    "                \n",
    "    def predict(self, X):\n",
    "        y_pred = np.round(self.sigmoid(X.dot(self.param))).astype(int)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891 891\n",
      "[ 1.08150087 -0.09606872 -0.8014497  -0.41642496  0.00278468  2.43560435\n",
      " -1.63962442  1.29037573  0.47316808 -0.75482874]\n"
     ]
    }
   ],
   "source": [
    "my_logistic_regression = MyLogisticRegression(fit_intercept=True, learning_rate=0.01, n_iteration=40000, gradient_descent=True)\n",
    "my_logistic_regression.fit(titanic_X, titanic_Y)\n",
    "print(my_logistic_regression.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[-0.19305985 -0.88827307 -0.41248478  0.25972605  2.77034198 -1.58282808\n",
    "   1.57403974  0.49032554 -0.87685138]] [1.18751389]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03948997, -0.35008063, -0.11333402,  0.00299111,  2.22210435,\n",
       "       -0.53882212,  1.61875327,  0.59564017, -0.53111121])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression1.param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[-0.05377125 -0.86185816 -0.38656617  0.06855475  3.45677599 -0.83105841\n",
    "   2.08641618  0.95124028 -0.41193888]] \n",
    "\n",
    "[[-0.03947528 -0.3498864  -0.11338565  0.00299138  2.22166324 -0.53911469\n",
    "   1.61848738  0.59533811 -0.53127694]]\n",
    "\n",
    "[-0.03948997 -0.35008063 -0.11333402  0.00299111  2.22210435 -0.53882212\n",
    "  1.61875327  0.59564017 -0.53111121]\n",
    "\n",
    "[-0.03948997 -0.35008063 -0.11333402  0.00299111  2.22210435 -0.53882212\n",
    "  1.61875327  0.59564017 -0.53111121]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### SKlearn testing ###############################\n",
    "logistic_regression = SGDClassifier(loss=\"log\", alpha=0, fit_intercept=True, max_iter=30000, learning_rate=\"constant\", eta0=0.001)\n",
    "logistic_regression.fit(titanic_X, titanic_Y)\n",
    "print(logistic_regression.coef_, logistic_regression.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression_batch = LogisticRegression(penalty=\"l2\",\n",
    "                                               fit_intercept=False,solver=\"liblinear\",\n",
    "                                               C=10000000,\n",
    "                                               max_iter=1000)\n",
    "logistic_regression_batch.fit(titanic_X, titanic_Y)\n",
    "print(logistic_regression_batch.coef_, logistic_regression_batch.intercept_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
